{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a sklearn Pipeline for a to ML contest submission\n",
    "In the ML_coruse_train notebook we at first analyzed the housing dataset to gain statistical insights and then e.g. features added new, \n",
    "replaced missing values and scaled the colums using pandas dataset methods.\n",
    "In the following we will use sklearn [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to integrate all these steps into one final *estimator*. The resulting pipeline can be used for saving an ML estimator to a file and use it later for production.\n",
    "\n",
    "*Optional:*\n",
    "If you want, you can save your estimator as explained in the last cell at the bottom of this notebook.\n",
    "Based on a hidden dataset, it's performance will then be ranked against all other submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read housing data again\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "housing = pd.read_csv(\"datasets/housing/housing.csv\")\n",
    "\n",
    "# Try to get header information of the dataframe:\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One remark: sklearn transformers do **not** act on pandas dataframes. Instead, they use numpy arrays.  \n",
    "Now try to [convert](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html) a dataframe to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the column names are lost now.\n",
    "In a numpy array, columns indexed using integers and no more by their names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add extra feature columns\n",
    "At first, we again add some extra columns (e.g. `rooms_per_household, population_per_household, bedrooms_per_household`) which might correlate better with the predicted parameter `median_house_value`.\n",
    "For modifying the dataset, we now use a [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html), which we later can put into a pipeline.  \n",
    "Hints:  \n",
    "* For finding the index number of a given column name, you can use the method [get_loc()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.get_loc.html)\n",
    "* For concatenating the new columns with the given array, you can use numpy method [c_](https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# At first, get the indexes as integers from the column names:\n",
    "rooms_ix = housing.columns.get_loc(\"total_rooms\")\n",
    "bedrooms_ix = housing.columns.get_loc(\"total_bedrooms\")\n",
    "population_ix = housing.columns.get_loc(\"population\")\n",
    "household_ix = housing.columns.get_loc(\"households\")\n",
    "\n",
    "# Now implement a function which takes a numpy array a argument and adds the new feature columns\n",
    "def add_extra_features(X):\n",
    "    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "    population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "    bedrooms_per_household = X[:, bedrooms_ix] / X[:, household_ix]\n",
    "    \n",
    "    # Concatenate the original array X with the new columns\n",
    "    return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_household]\n",
    "\n",
    "attr_adder = FunctionTransformer(add_extra_features, validate = False)\n",
    "housing_extra_attribs = attr_adder.fit_transform(housing.values)\n",
    "\n",
    "assert housing_extra_attribs.shape == (16512, 13)\n",
    "housing_extra_attribs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing elements\n",
    "For replacing nan values in the dataset with the mean or median of the column they are in, you can also use a [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# Drop the categorial column ocean_proximity\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "\n",
    "print(\"We have %d nan elements in the numerical columns\" %np.count_nonzero(np.isnan(housing_num.values)))\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "housing_num_cleaned = imp_mean.fit_transform(housing_num)\n",
    "\n",
    "assert np.count_nonzero(np.isnan(housing_num_cleaned)) == 0\n",
    "housing_num_cleaned[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column scaling\n",
    "For scaling the columns, you can use a [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(housing_num_cleaned)\n",
    "print(\"mean of the columns is: \" ,  np.mean(scaled, axis=0))\n",
    "print(\"standard deviation of the columns is: \" ,  np.std(scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all preprocessing steps together  \n",
    "Now let's build a pipeline for preprocessing the **numerical** attributes.\n",
    "The pipeline shall process the data in the following steps:\n",
    "* [Impute](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) median or mean values for elements which are NaN\n",
    "* Add attributes using the FunctionTransformer with the function add_extra_features().\n",
    "* Scale the numerical values using the [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# Now test the pipeline on housing_num\n",
    "num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a pipeline for the numerical columns.  \n",
    "But we still have a categorical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need one more pipeline for the categorical column. Instead of the \"Dummy encoding\" we used before, we now use the [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "cat_encoder = OneHotEncoder(sparse = False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need for building a preprocessing pipeline which transforms the columns including all the steps before.\n",
    "Since we have columns where different transformations should be applied, we use the class [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\", \"total_bedrooms\",\n",
    "               \"population\",\"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_prep_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "full_prep_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an estimator\n",
    "Include `full_prep_pipeline` into a further pipeline where it is followed by an RandomForestRegressor.\n",
    "This way, at first our data is prepared using `full_prep_pipeline` and then the RandomForestRegressor is trained on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_prep_pipeline),\n",
    "        (\"forest\", RandomForestRegressor())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, seperate the label colum (`median_house_value`) and feature columns (all other columns).\n",
    "Split the data into a training and testing dataset using train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two dataframes, one for the labels one for the features\n",
    "housing_features = housing.drop(\"median_house_value\", axis = 1)\n",
    "housing_labels = housing[\"median_house_value\"]\n",
    "\n",
    "# Split the two dataframes into a training and a test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_features, housing_labels, test_size = 0.20)\n",
    "\n",
    "# Now train the full_pipeline_with_predictor on the training dataset\n",
    "full_pipeline_with_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, calculate some score metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = full_pipeline_with_predictor.predict(X_test)\n",
    "tree_mse = mean_squared_error(y_pred, y_test)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [pickle serializer](https://docs.python.org/3/library/pickle.html) to save your estimator to a file for contest participation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import getpass\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "your_regressor = full_pipeline_with_predictor\n",
    "assert isinstance(your_regressor, Pipeline)\n",
    "pickle.dump(your_regressor, open(getpass.getuser() + \"s_model.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_valid = pd.read_csv(\"datasets/housing/housing_valid.csv\")\n",
    "housing_valid_features = housing_valid.drop(\"median_house_value\", axis = 1)\n",
    "housing_valid_labels = housing_valid[\"median_house_value\"]\n",
    "housing_valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chrus_model.p', 'rb') as handle:\n",
    "    contestModel = pickle.load(handle)\n",
    "    y_pred = contestModel.predict(housing_valid_features)\n",
    "    tree_mse = mean_squared_error(y_pred, housing_valid_labels.to_numpy())\n",
    "    tree_rmse = np.sqrt(tree_mse)\n",
    "    print(tree_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating hidden test set\n",
    "housing_orig = pd.read_csv(\"datasets/housing/housing_orig.csv\")\n",
    "course, valid = train_test_split(housing_orig, test_size=0.2, random_state = 26)\n",
    "valid.to_csv(\"datasets/housing/housing_valid.csv\", index = False)\n",
    "course.to_csv(\"datasets/housing/housing.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
